{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":448151,"sourceType":"datasetVersion","datasetId":204007}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"include_colab_link":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/hoangvu7986/dog-cat-classification/blob/main/miai_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"# List folder for demo\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"SERJH0mGJEb9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List GPUs\nfrom tensorflow.python.client import device_lib\n\ndef get_available_gpus():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n\nprint(get_available_gpus())","metadata":{"trusted":true,"id":"AkAk42tqJEb_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download file from GDrive. Shoudld use zip file and unzip later\n\n!pip install gdown\n!gdown --id 1NQ8BDtsg0GbHJw9bX5ceq_u3_56ne8n2","metadata":{"trusted":true,"id":"1gg_MTkOJEcB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download from Remote URL\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n    -O /tmp/cats_and_dogs_filtered.zip","metadata":{"trusted":true,"id":"qccGd5DNJEcC"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\n\nlocal_zip = '/tmp/cats_and_dogs_filtered.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('/tmp')\nzip_ref.close()","metadata":{"trusted":true,"id":"uZGrWCc7JEcC"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_dir = '/tmp/cats_and_dogs_filtered'\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'validation')\n\n# Directory with our training cat pictures\ntrain_cats_dir = os.path.join(train_dir, 'cats')\n\n# Directory with our training dog pictures\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\n\n# Directory with our validation cat pictures\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\n\n# Directory with our validation dog pictures\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')","metadata":{"trusted":true,"id":"Ij4EL8phJEcD"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_cat_fnames = os.listdir(train_cats_dir)\nprint(train_cat_fnames[:10])\n\ntrain_dog_fnames = os.listdir(train_dogs_dir)\ntrain_dog_fnames.sort()\nprint(train_dog_fnames[:10])","metadata":{"trusted":true,"id":"Bzcv9YONJEcD"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('total training cat images:', len(os.listdir(train_cats_dir)))\nprint('total training dog images:', len(os.listdir(train_dogs_dir)))\nprint('total validation cat images:', len(os.listdir(validation_cats_dir)))\nprint('total validation dog images:', len(os.listdir(validation_dogs_dir)))","metadata":{"trusted":true,"id":"xnp7457cJEcD"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Parameters for our graph; we'll output images in a 4x4 configuration\nnrows = 4\nncols = 4\n\n# Index for iterating over images\npic_index = 0","metadata":{"trusted":true,"id":"iEnLlKcMJEcE"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols * 4, nrows * 4)\n\npic_index += 8\nnext_cat_pix = [os.path.join(train_cats_dir, fname)\n                for fname in train_cat_fnames[pic_index-8:pic_index]]\nnext_dog_pix = [os.path.join(train_dogs_dir, fname)\n                for fname in train_dog_fnames[pic_index-8:pic_index]]\n\nfor i, img_path in enumerate(next_cat_pix+next_dog_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()\n","metadata":{"trusted":true,"id":"ZiLa7gvJJEcE"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras import Model","metadata":{"trusted":true,"id":"YWjnpF5bJEcF"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n# the three color channels: R, G, and B\nimg_input = layers.Input(shape=(150, 150, 3))\n\n# First convolution extracts 16 filters that are 3x3\n# Convolution is followed by max-pooling layer with a 2x2 window\nx = layers.Conv2D(16, 3, activation='relu')(img_input)\nx = layers.MaxPooling2D(2)(x)\n\n# Second convolution extracts 32 filters that are 3x3\n# Convolution is followed by max-pooling layer with a 2x2 window\nx = layers.Conv2D(32, 3, activation='relu')(x)\nx = layers.MaxPooling2D(2)(x)\n\n# Third convolution extracts 64 filters that are 3x3\n# Convolution is followed by max-pooling layer with a 2x2 window\nx = layers.Conv2D(64, 3, activation='relu')(x)\nx = layers.MaxPooling2D(2)(x)","metadata":{"trusted":true,"id":"u6jQ7CBIJEcF"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Flatten feature map to a 1-dim tensor so we can add fully connected layers\nx = layers.Flatten()(x)\n\n# Create a fully connected layer with ReLU activation and 512 hidden units\nx = layers.Dense(512, activation='relu')(x)\n\n# Create output layer with a single node and sigmoid activation\noutput = layers.Dense(1, activation='sigmoid')(x)\n\n# Create model:\n# input = input feature map\n# output = input feature map + stacked convolution/maxpooling layers + fully\n# connected layer + sigmoid output layer\nmodel = Model(img_input, output)","metadata":{"trusted":true,"id":"Abgac3YOJEcF"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=RMSprop(learning_rate=0.001),\n              metrics=['acc'])","metadata":{"trusted":true,"id":"rC6oX5rTJEcF"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# All images will be rescaled by 1./255\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n# Flow training images in batches of 20 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(\n        train_dir,  # This is the source directory for training images\n        target_size=(150, 150),  # All images will be resized to 150x150\n        batch_size=20,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary')\n\n# Flow validation images in batches of 20 using val_datagen generator\nvalidation_generator = val_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(150, 150),\n        batch_size=20,\n        class_mode='binary')","metadata":{"trusted":true,"id":"IurpGYnGJEcG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n      train_generator,\n      steps_per_epoch=100,  # 2000 images = batch_size * steps\n      epochs=15,\n      validation_data=validation_generator,\n      validation_steps=50,  # 1000 images = batch_size * steps\n      verbose=2)","metadata":{"trusted":true,"id":"4iXEN6DvJEcG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"catdogmodel.h5\")","metadata":{"trusted":true,"id":"cY9de5ylJEcG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retrieve a list of accuracy results on training and validation data\n# sets for each training epoch\nacc = history.history['acc']\nval_acc = history.history['val_acc']\n\n# Retrieve a list of list results on training and validation data\n# sets for each training epoch\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# Get number of epochs\nepochs = range(len(acc))\n\n# Plot training and validation accuracy per epoch\nplt.plot(epochs, acc)\nplt.plot(epochs, val_acc)\nplt.title('Training and validation accuracy')\n\nplt.figure()\n\n# Plot training and validation loss per epoch\nplt.plot(epochs, loss)\nplt.plot(epochs, val_loss)\nplt.title('Training and validation loss')","metadata":{"trusted":true,"id":"JC_WjwM_JEcH"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, signal\nos.kill(os.getpid(), signal.SIGKILL)","metadata":{"trusted":true,"id":"t3XhjzjYJEcH"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true,"id":"AnyogngxJEcH"},"outputs":[],"execution_count":null}]}